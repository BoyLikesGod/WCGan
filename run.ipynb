{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.listdir(os.getcwd())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)-15s %(levelname)s: %(message)s')\n",
    "seed = 12345\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.dataset = 'pheme9'\n",
    "        self.data_numworker = 4  # 加载数据集时的线程\n",
    "        self.train_batch = 16  # 训练集的batch大小\n",
    "        self.test_batch = 16  # 测试集的batch大小\n",
    "        self.epochs = 500  # 训练迭代次数\n",
    "        # 词向量\n",
    "        self.pad_size = 288  # 文本长度 pheme:288\n",
    "        self.embedding_dim = 100\n",
    "        self.num_class = 2  # 分类的类别数量\n",
    "        # gan config\n",
    "        self.lr_g = 0.001\n",
    "        self.lr_d = 0.01\n",
    "        # 训练判别器时，生成数据在判别器上的损失权重\n",
    "        self.ge_weight = 0.5\n",
    "        self.re_weight = 0\n",
    "        # 训练生成器的损失权重\n",
    "        self.id_weight = 5\n",
    "        self.cycle_weight = 10\n",
    "        # generator config\n",
    "        self.gen_rnn_hidden_size = self.embedding_dim\n",
    "        self.gen_rnn_num_layers = 2\n",
    "        self.gen_rnn_dropout = 0.4\n",
    "        # discriminator config\n",
    "        self.dis_rnn_hidden_size = self.embedding_dim  # LSTM中隐藏层的大小\n",
    "        self.dis_rnn_lstm_layers = 2  # RNN中LSTM的层数\n",
    "        self.dis_rnn_dropout = 0.4  # LSTM中的dropout rate\n",
    "config = Config()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_device(gpu):\n",
    "    USE_CUDA = torch.cuda.is_available()\n",
    "    device = torch.device(gpu if USE_CUDA else 'cpu')\n",
    "    logging.info(\"GPU: %s, use %s\", USE_CUDA, device)\n",
    "    return device\n",
    "config.device = get_device('cuda:0')\n",
    "print(f\"dataset: {config.dataset}\", f\"pad size: {config.pad_size}\", sep='\\n-->')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_vocab_and_embedding(dataset):\n",
    "    embedding_path = f'data/{dataset}_embedding.npz'\n",
    "    vocab_path = f'data/{dataset}_vocab.pkl'\n",
    "    with open(vocab_path, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "        vocab_size = len(vocab)\n",
    "    pretrained_embedding = torch.tensor(np.load(embedding_path, allow_pickle=True)['embeddings'].astype(\"float32\"))\n",
    "    embedding_dim = pretrained_embedding.size(1)\n",
    "    print('', f'vocab size: {vocab_size}', f'embedding dim: {embedding_dim}', sep='\\n-->')\n",
    "    return vocab, vocab_size, pretrained_embedding\n",
    "config.vocab, config.vocab_size, config.pretrained = load_vocab_and_embedding(config.dataset)\n",
    "config.embedding_dim = config.pretrained.size(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_post(content, vocab, pad_size, unk, pad):\n",
    "    post = []\n",
    "    for word in content['post'].split(' '):\n",
    "        idx = vocab.index(word) if word in vocab else unk\n",
    "        post.append(idx)\n",
    "    if len(post) < pad_size:\n",
    "        pad_width = pad_size - len(post)\n",
    "        post = np.pad(post, (0, pad_width), mode='constant', constant_values=pad)\n",
    "    elif len(post) > pad_size:\n",
    "        post = post[:pad_size]\n",
    "    return post\n",
    "def get_label(label):\n",
    "    mask = torch.zeros(2).float()\n",
    "    mask[int(label)] = 1.0\n",
    "    return mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class buildDataset(data.Dataset):\n",
    "    def __init__(self, config, mode):\n",
    "        super(buildDataset, self).__init__()\n",
    "        self.index = f'data/{config.dataset}_index.pkl'\n",
    "        self.dataset = f'data/{config.dataset}_data.pkl'\n",
    "        self.pad_size = config.pad_size  # 句子长度\n",
    "        self.vocab = config.vocab  # 词表，一个列表，存放着所有的词\n",
    "        self.unk = config.vocab_size - 2\n",
    "        self.pad = config.vocab_size - 1\n",
    "        self.data = []\n",
    "        with open(self.index, 'rb') as f:\n",
    "            index = pickle.load(f)[mode]\n",
    "        with open(self.dataset, 'rb') as f:\n",
    "            datasets = pickle.load(f)\n",
    "        \"\"\"data_index = dataset_index\"\"\"\n",
    "        random.shuffle(index)\n",
    "        for post in index:\n",
    "            self.data.append(datasets[post])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        content = self.data[index]\n",
    "        post = get_post(content, self.vocab, self.pad_size, self.unk, self.pad)\n",
    "        label, length, name = content['label'], content['length'], content['post name']\n",
    "        post = torch.tensor(post).type(torch.int32)\n",
    "        label = get_label(label)\n",
    "        return post, label, length, name\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rumor_train_dataset = buildDataset(config, 'rumor_train')\n",
    "norumor_train_dataset = buildDataset(config, 'norumor_train')\n",
    "test_dataset = buildDataset(config, 'test')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def data_collate(batch):\n",
    "    post = [item[0].numpy() for item in batch]\n",
    "    label = [item[1].numpy() for item in batch]\n",
    "    length = [item[2] for item in batch]\n",
    "    name = [item[3] for item in batch]\n",
    "    post = torch.tensor(np.array(post), dtype=torch.int)\n",
    "    label = torch.tensor(np.array(label), dtype=torch.float32)\n",
    "    return post, label, length, name"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rumor_train_dataloader = data.DataLoader(rumor_train_dataset, batch_size=config.train_batch,num_workers=config.data_numworker, shuffle=True,collate_fn=data_collate, drop_last=True)\n",
    "norumor_train_dataloader = data.DataLoader(norumor_train_dataset, batch_size=config.train_batch,num_workers=config.data_numworker, shuffle=True,collate_fn=data_collate, drop_last=True)\n",
    "test_dataloader = data.DataLoader(test_dataset, batch_size=config.test_batch,num_workers=config.data_numworker, shuffle=True,collate_fn=data_collate, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def draw_f1(train_f1, test_f1, message, save_path):\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.title('f1_score')\n",
    "    plt.plot(train_f1, label='train f1', linewidth=1, color='red')\n",
    "    plt.plot(test_f1, label='test f1', linewidth=1, color='black')\n",
    "    plt.xlabel(message)\n",
    "    train_max = np.argmax(train_f1)\n",
    "    test_max = np.argmax(test_f1)\n",
    "    show_train = '[' + str(train_max) + '  ' + str(train_f1[train_max]) + ']'\n",
    "    plt.plot(train_max, train_f1[train_max], 'ro')\n",
    "    plt.annotate(show_train, xy=(train_max, train_f1[train_max]),xytext=(train_max, train_f1[train_max]))\n",
    "    show_test = '[' + str(test_max) + '  ' + str(test_f1[test_max]) + ']'\n",
    "    plt.plot(test_max, test_f1[test_max], 'ko')\n",
    "    plt.annotate(show_test, xy=(test_max, test_f1[test_max]), xytext=(test_max, test_f1[test_max]))\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + '/' + 'f1_score.jpg')\n",
    "def draw_acc(train_acc, test_acc, message, save_path):\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.title('acc')\n",
    "    plt.plot(train_acc, label='train acc', linewidth=1, color='red')\n",
    "    plt.plot(test_acc, label='test acc', linewidth=1, color='black')\n",
    "    plt.xlabel(message)\n",
    "    train_max = np.argmax(train_acc)\n",
    "    test_max = np.argmax(test_acc)\n",
    "    show_train = '[' + str(train_max) + '  ' + str(train_acc[train_max]) + ']'\n",
    "    plt.plot(train_max, train_acc[train_max], 'ro')\n",
    "    plt.annotate(show_train, xy=(train_max, train_acc[train_max]),xytext=(train_max, train_acc[train_max]))\n",
    "    show_test = '[' + str(test_max) + '  ' + str(test_acc[test_max]) + ']'\n",
    "    plt.plot(test_max, test_acc[test_max], 'ko')\n",
    "    plt.annotate(show_test, xy=(test_max, test_acc[test_max]), xytext=(test_max, test_acc[test_max]))\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + '/' + 'acc.jpg')\n",
    "def draw_target(train_acc, test_acc, train_f1, test_f1, name, message):\n",
    "    current_time = str(get_current_time())\n",
    "    save_path = f\"result/{name}\"\n",
    "    if not os.path.isdir(save_path): os.mkdir(save_path)\n",
    "    save_path = save_path + \"/\" + current_time\n",
    "    if not os.path.isdir(save_path): os.mkdir(save_path)\n",
    "    draw_acc(train_acc, test_acc, message, save_path)\n",
    "    draw_f1(train_f1, test_f1, message, save_path)\n",
    "    return save_path"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_scheduler(opt, sc_mode='min', sc_factor=0.8, sc_patience=10, sc_verbose=True, sc_threshold=0.0001,sc_threshold_mode='rel', sc_cooldown=0, sc_min_lr=0, sc_eps=1e-8):\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=sc_mode, factor=sc_factor, patience=sc_patience,verbose=sc_verbose, threshold=sc_threshold,\n",
    "                                                           threshold_mode=sc_threshold_mode, cooldown=sc_cooldown,min_lr=sc_min_lr, eps=sc_eps)\n",
    "    return scheduler\n",
    "def neg_label(label):\n",
    "    b = torch.ones_like(label)\n",
    "    label = b + torch.neg(label)\n",
    "    return label\n",
    "class WrongLabelLogger:\n",
    "    def __init__(self):\n",
    "        self.logger = {}\n",
    "    def log(self, label_index, out_index, data):\n",
    "        wrong_index = label_index - out_index\n",
    "        wrong_index.astype(int)\n",
    "        for index, label in enumerate(wrong_index):\n",
    "            if label != 0:\n",
    "                if data[index] not in self.logger: self.logger[data[index]] = 1\n",
    "                else: self.logger[data[index]] += 1\n",
    "    def write(self, path):\n",
    "        path = path + '/logger.json'\n",
    "        with open(path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.logger, f, indent=2, ensure_ascii=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Generator, self).__init__()\n",
    "        self.config = config\n",
    "        self.lstm = nn.LSTM(config.embedding_dim, config.gen_rnn_hidden_size, config.gen_rnn_num_layers, bias=True, bidirectional=True, batch_first=True, dropout=config.gen_rnn_dropout).to(self.config.device)\n",
    "        self.fc = nn.Linear(config.gen_rnn_hidden_size * 2, config.embedding_dim).to(self.config.device)\n",
    "    def forward(self, x, state=None):\n",
    "        batch, _, _ = x.size()\n",
    "        if state is None:\n",
    "            h = torch.randn(self.config.gen_rnn_num_layers * 2, batch, self.config.gen_rnn_hidden_size).float().to(self.config.device)\n",
    "            c = torch.randn(self.config.gen_rnn_num_layers * 2, batch, self.config.gen_rnn_hidden_size).float().to(self.config.device)\n",
    "        else: h, c = state\n",
    "        out, state = self.lstm(x, (h, c))\n",
    "        out = self.fc(out)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, embedding_dim, device):\n",
    "        super(Attention, self).__init__()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.w = nn.Parameter(torch.randn(embedding_dim, requires_grad=True)).to(device)\n",
    "        self.w.data.normal_(mean=0.0, std=0.05)\n",
    "    def forward(self, x):\n",
    "        M = self.tanh(x)\n",
    "        alpha = F.softmax(torch.matmul(M, self.w), dim=1).unsqueeze(-1)\n",
    "        out = x * alpha\n",
    "        return out\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding.from_pretrained(config.pretrained, freeze=False).to(config.device)\n",
    "        self.att = Attention(config.embedding_dim, config.device)\n",
    "        self.lstm = nn.LSTM(config.embedding_dim * 2, config.dis_rnn_hidden_size, config.dis_rnn_lstm_layers, bias=True, bidirectional=True, batch_first=True, dropout=config.dis_rnn_dropout).to(config.device)\n",
    "        self.fc = nn.Linear(config.dis_rnn_hidden_size * 2, config.num_class).to(config.device)\n",
    "    def forward(self, emb, state=None):\n",
    "        batch, pad_size, embedding_size = emb.size()\n",
    "        if state is None:\n",
    "            h = torch.randn(self.config.dis_rnn_lstm_layers * 2, batch, self.config.dis_rnn_hidden_size).float().to(self.config.device)\n",
    "            c = torch.randn(self.config.dis_rnn_lstm_layers * 2, batch, self.config.dis_rnn_hidden_size).float().to(self.config.device)\n",
    "        else: h, c = state\n",
    "        out = self.att(emb)\n",
    "        out = torch.cat((out, emb), -1)\n",
    "        out, state = self.lstm(out, (h, c))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class WCGan(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(WCGan, self).__init__()\n",
    "        self.config = config\n",
    "        self.mes = f'dataset: {config.dataset}  pad size: {config.pad_size}  embedding dim: {config.embedding_dim}  dis weight:[{config.ge_weight}, {config.re_weight}]  gen weight:[{config.id_weight}, {config.cycle_weight}]'\n",
    "        print('\\n'.join(self.mes.split('  ')))\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(config.pretrained, freeze=False).to(self.config.device)\n",
    "        self.G_r = Generator(self.config)   # 谣言生成器\n",
    "        self.G_n = Generator(self.config)   # 非谣言生成器\n",
    "        self.D = Discriminator(self.config) # 判别器\n",
    "\n",
    "        self.adv_loss = nn.BCEWithLogitsLoss().to(self.config.device)\n",
    "        # 优化器\n",
    "        self.G_n_optimizer = torch.optim.RMSprop(self.G_n.parameters(), lr=config.lr_g)\n",
    "        self.G_r_optimizer = torch.optim.RMSprop(self.G_r.parameters(), lr=config.lr_g)\n",
    "        self.D_optimizer = torch.optim.RMSprop(self.D.parameters(), lr=config.lr_d)\n",
    "\n",
    "        self.train_acc, self.test_acc = [], []\n",
    "        self.train_f1, self.test_f1 = [], []\n",
    "        self.loss_d, self.loss_gr, self.loss_gn, self.loss_g = 0, 0, 0, 0\n",
    "\n",
    "        self.wrong_label_logger = WrongLabelLogger()\n",
    "\n",
    "    def forward(self, norumor_train_dataloader, rumor_train_dataloader, test_dataloader):\n",
    "        scheduler_D = get_scheduler(self.D_optimizer)\n",
    "        scheduler_Gr = get_scheduler(self.G_r_optimizer, sc_patience=8)\n",
    "        scheduler_Gn = get_scheduler(self.G_n_optimizer, sc_patience=8)\n",
    "        for epoch in range(self.config.epochs):\n",
    "            print(f'----------------------------------Epoch : {epoch + 1}----------------------------------')\n",
    "            loop = tqdm(enumerate(zip(norumor_train_dataloader, rumor_train_dataloader)), total=min(len(norumor_train_dataloader), len(rumor_train_dataloader)))\n",
    "            loop.set_description(f\"Training--[Epoch {epoch + 1} : {self.config.epochs}]\")\n",
    "            predict, true = np.array([]), np.array([])\n",
    "            flag = torch.randint(0, 2, [1])  # 0:norumor 1:rumor\n",
    "            for i, data in loop:\n",
    "                flag = (flag + 1) % 2\n",
    "                or_post, or_label = data[flag][0].to(self.config.device), data[flag][1].to(self.config.device)\n",
    "                or_post = self.embedding(or_post)  # embedding，将词序列转换成词向量\n",
    "                if i % 3 != 0: self.train_discriminator(flag, or_post, or_label)\n",
    "                else: self.train_generator(flag, data)\n",
    "                with torch.no_grad():\n",
    "                    out = self.D(or_post)\n",
    "                true_label = torch.argmax(or_label, 1).cpu().numpy()\n",
    "                out_label = torch.argmax(out, 1).cpu().numpy()\n",
    "                true = np.append(true, true_label)\n",
    "                predict = np.append(predict, out_label)\n",
    "                self.wrong_label_logger.log(true_label, out_label, data[flag][3])\n",
    "\n",
    "                acc = metrics.accuracy_score(true, predict)\n",
    "                f1 = metrics.f1_score(true, predict, average=None)\n",
    "                f1_s = metrics.f1_score(true, predict, average='macro')\n",
    "                loop.set_postfix(acc=format(acc, '.5f'), f1=f'[{f1_s}, {f1}]', loss=\"[{:.4f}|{:.4f}]\".format(self.loss_d, self.loss_g))\n",
    "            test_acc, test_f1, test_loss = self.test(test_dataloader, epoch)\n",
    "            self.train_acc.append(acc)\n",
    "            self.test_acc.append(test_acc)\n",
    "            self.train_f1.append(f1_s)\n",
    "            self.test_f1.append(test_f1)\n",
    "\n",
    "            scheduler_D.step(self.loss_d)\n",
    "            scheduler_Gr.step(self.loss_gr)\n",
    "            scheduler_Gn.step(self.loss_gn)\n",
    "\n",
    "        save_path = draw_target(self.train_acc, self.test_acc, self.train_f1, self.test_f1, self.config.model_name, self.mes)\n",
    "        self.wrong_label_logger.write(save_path)\n",
    "\n",
    "    def train_discriminator(self, flag, or_post, or_label):\n",
    "        self.D_optimizer.zero_grad()\n",
    "        or_out = self.D(or_post)\n",
    "        or_loss = self.adv_loss(or_out, or_label)\n",
    "        if flag == 0:  # norumor\n",
    "            ge_post = self.G_r(or_post)\n",
    "            re_post = self.G_n(ge_post)\n",
    "        else:  # rumor\n",
    "            ge_post = self.G_n(or_post)\n",
    "            re_post = self.G_r(or_post)\n",
    "        ge_out = self.D(ge_post)\n",
    "        ge_loss = self.adv_loss(ge_out, neg_label(or_label))\n",
    "        re_out = self.D(re_post)\n",
    "        re_loss = self.adv_loss(re_out, or_label)\n",
    "        loss_d = or_loss + ge_loss * self.config.ge_weight + re_loss * self.config.re_weight\n",
    "        loss_d.backward(retain_graph=True)\n",
    "        self.loss_d = loss_d\n",
    "        self.D_optimizer.step()\n",
    "\n",
    "    def train_generator(self, flag, data):\n",
    "        or_post, or_label = data[flag][0].to(self.config.device), data[flag][1].to(self.config.device)\n",
    "        or_post = self.embedding(or_post)  # embedding，将词序列转换成词向量\n",
    "        an_post = data[(flag + 1) % 2][0].to(self.config.device)\n",
    "        an_post = self.embedding(an_post)\n",
    "        if flag == 0:  # norumor\n",
    "            self.G_r_optimizer.zero_grad()\n",
    "            id_post = self.G_r(an_post)\n",
    "            ge_post = self.G_r(or_post)\n",
    "            re_post = self.G_n(ge_post)\n",
    "        else:  # rumor\n",
    "            self.G_n_optimizer.zero_grad()\n",
    "            id_post = self.G_n(an_post)\n",
    "            ge_post = self.G_n(or_post)\n",
    "            re_post = self.G_r(ge_post)\n",
    "        re_out = self.D(re_post)\n",
    "        # loss_id = -torch.mean(self.D(an_post)) + torch.mean(self.D(id_post))\n",
    "        loss_id = -torch.mean(an_post) + torch.mean(id_post)\n",
    "        # loss_id = self.adv_loss(self.D(id_post), neg_label(or_label))\n",
    "        loss_ge = -torch.mean(self.D(or_post)) + torch.mean(self.D(ge_post))\n",
    "        loss_cy = self.adv_loss(re_out, or_label)\n",
    "        # loss_cy = -torch.mean(self.D(or_post)) + torch.mean(self.D(re_post))\n",
    "        loss_g = loss_ge + loss_id * self.config.id_weight + loss_cy * self.config.cycle_weight\n",
    "        loss_g.backward(retain_graph=True)\n",
    "        self.loss_g = loss_g\n",
    "        self.D.zero_grad()\n",
    "        if flag == 0:\n",
    "            self.loss_gr = loss_g\n",
    "            self.G_n.zero_grad()\n",
    "            self.G_r_optimizer.step()\n",
    "        else:\n",
    "            self.loss_gn = loss_g\n",
    "            self.G_r.zero_grad()\n",
    "            self.G_n_optimizer.step()\n",
    "    def test(self, dataloader, epoch):\n",
    "        loop = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "        loop.set_description(f\"Testing---[Epoch {epoch + 1} : {self.config.epochs}]\")\n",
    "        predicts_all, labels_all = np.array([], dtype=int), np.array([], dtype=int)\n",
    "        loss_all = 0\n",
    "        with torch.no_grad():\n",
    "            for i, data in loop:\n",
    "                post, label = data[0].to(self.config.device), data[1].to(self.config.device)\n",
    "                post = self.embedding(post)\n",
    "                out = self.D(post)\n",
    "                loss = self.adv_loss(out, label)\n",
    "                loss_all = loss_all + loss\n",
    "                labels = torch.argmax(label, 1).data.cpu().numpy()\n",
    "                predicts = torch.argmax(out.data, 1).cpu().numpy()\n",
    "                labels_all = np.append(labels_all, labels)\n",
    "                predicts_all = np.append(predicts_all, predicts)\n",
    "                self.wrong_label_logger.log(labels, predicts, data[3])\n",
    "                acc = metrics.accuracy_score(labels_all, predicts_all)\n",
    "                f1 = metrics.f1_score(labels_all, predicts_all, average=None)\n",
    "                f1_s = metrics.f1_score(labels_all, predicts_all, average='macro')\n",
    "                total_loss = (loss_all / i).item()\n",
    "\n",
    "                loop.set_postfix(acc=format(acc, '.5f'), f1=f'[{f1_s}, {f1}]',test_loss=format(total_loss, '.4f'))\n",
    "        return acc, f1_s, total_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = WCGan(config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model(norumor_train_dataloader, rumor_train_dataloader, test_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}